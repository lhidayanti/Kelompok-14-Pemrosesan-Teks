{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVkdT25c3W7"
      },
      "source": [
        "# Mengimpor dataset MRT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt5UN4jqTfMs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjEPfmDdToaq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nphzbwz-T55Y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Pemrosesan Teks Revisi/SrappingMRT (1).csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foZVaCZlUDaj"
      },
      "outputs": [],
      "source": [
        "df = df[['full_text']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1oZUWWpeJnE"
      },
      "source": [
        "#Preprocessing Sederhana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91LPx_uqUwPb"
      },
      "source": [
        "## Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27yreo1VU05m"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_basic(text):\n",
        "    text = str(text).lower()  # ke huruf kecil\n",
        "    text = re.sub(r'@\\w+', '', text)  # hapus mention\n",
        "    text = re.sub(r'(?:http?://|https?://|www\\.)\\S+', '', text)  # hapus url\n",
        "    text = re.sub(r'\\d+', '', text)  # hapus angka\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # huruf berulang\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # hapus tanda baca\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # rapikan spasi\n",
        "\n",
        "    return text\n",
        "\n",
        "df['punctuation_text'] = df['full_text'].apply(normalize_basic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXQ8nrbtW2dM"
      },
      "source": [
        "##Normalisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_p1HYRuCa1K"
      },
      "outputs": [],
      "source": [
        "# !pip install indoNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL1kxWacAole"
      },
      "outputs": [],
      "source": [
        "# from indoNLP.preprocessing import replace_slang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW9CI7Ft_sOu"
      },
      "outputs": [],
      "source": [
        "kamus_path = pd.read_csv('/content/drive/MyDrive/Pemrosesan Teks Revisi/slang_indo.csv', header=None, names=[\"slang\",\"formal\"])\n",
        "kamus_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXglCZG6CDbB"
      },
      "outputs": [],
      "source": [
        "slang_dict = dict(zip(kamus_path['slang'], kamus_path['formal']))\n",
        "\n",
        "def normalize_slang(text):\n",
        "    tokens = str(text).split()\n",
        "    normalized = [slang_dict.get(tok, tok) for tok in tokens]\n",
        "    return ' '.join(normalized)\n",
        "\n",
        "df['normalized'] = df['punctuation_text'].astype(str).apply(normalize_slang)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IAYqSQCUILb"
      },
      "source": [
        "# Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znogzJOsUKVe"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cWDiv5OWD5h"
      },
      "outputs": [],
      "source": [
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\",\n",
        "    tokenizer=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4MTkWypUfnM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "df['hf_label'] = df['normalized'].apply(lambda x: sentiment_pipe(x)[0]['label'])\n",
        "df['hf_score'] = df['normalized'].apply(lambda x: sentiment_pipe(x)[0]['score'])\n",
        "df.to_csv('labeling.csv', index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJQANLNTUkkC"
      },
      "outputs": [],
      "source": [
        "print(\"HuggingFace label counts:\")\n",
        "print(df['hf_label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7wA-Grbut20",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Pemrosesan Teks Revisi/final labeling bagus.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmtGZNjSE0Ov"
      },
      "outputs": [],
      "source": [
        "print(\"HuggingFace label counts:\")\n",
        "print(df['hf_final'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAFedTKWPeYM"
      },
      "source": [
        "#Preprocessing Lanjutan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGW20fzQdT7i"
      },
      "source": [
        "##Tokenisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGViu9BCWbJ8"
      },
      "outputs": [],
      "source": [
        "df['tokens'] = df['normalized'].apply(lambda x: x.split())\n",
        "df['tokens'] = df['tokens'].apply(lambda toks: [t for t in toks if len(t) > 1])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwpbmCjId_mQ"
      },
      "source": [
        "##Stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZiOTgwIBO1D"
      },
      "outputs": [],
      "source": [
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W32f9loYYK7r"
      },
      "outputs": [],
      "source": [
        "manual_stopwords = [\n",
        "    'yang', 'dan', 'di', 'ke', 'dari', 'itu', 'ini',\n",
        "    'untuk', 'pada', 'dengan', 'karena', 'bahwa', 'saat',\n",
        "    'ada', 'tidak', 'ya', 'nih', 'loh', 'sih', 'agar', 'atau',\n",
        "    'sehingga', 'tersebut', 'eh','akan','aku','bisa','dalam','dari','dia',\n",
        "    'dong','jadi','kalau','kalo','kan','kau','kita', 'lagi','lah',\n",
        "    'loh','me','mereka','nih','nya','para','pun','sama','saat','sebuah',\n",
        "    'seorang','seperti','sudah','telah','tidak','ya','yang','yg',\n",
        "]\n",
        "\n",
        "def remove_manual_stopwords(tokens_str):\n",
        "    try:\n",
        "        tokens = ast.literal_eval(tokens_str)\n",
        "    except (ValueError, SyntaxError):\n",
        "        tokens = [tokens_str]\n",
        "\n",
        "    # hapus kata yang ada di daftar stopword manual\n",
        "    filtered = [word for word in tokens if word not in manual_stopwords]\n",
        "    return filtered\n",
        "\n",
        "# Terapkan ke kolom\n",
        "df['stopword'] = df['tokens'].apply(remove_manual_stopwords)\n",
        "df[['tokens', 'stopword']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJgFi9gveDK3"
      },
      "source": [
        "##Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxxrOXJACmnj"
      },
      "outputs": [],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpxMCZNABFaN"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YwMGMfpZdgc"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KombpxHCRcW6"
      },
      "outputs": [],
      "source": [
        "def apply_stemming(tokens):\n",
        "    if isinstance(tokens, str):\n",
        "        tokens = tokens.split()\n",
        "    # Flatten satu level dan filter string\n",
        "    flat_tokens = list(itertools.chain.from_iterable(t if isinstance(t, list) else [t] for t in tokens))\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in flat_tokens if isinstance(word, str)]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "# Terapkan seperti sebelumnya\n",
        "df['stemmed'] = df['stopword'].apply(apply_stemming)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud"
      ],
      "metadata": {
        "id": "_TjXRcxqr7y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9DllswZJPKp"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2X_ZD9HJKFP"
      },
      "outputs": [],
      "source": [
        "# word plot semua\n",
        "all_words = \" \".join(df['stemmed'])\n",
        "\n",
        "wc = WordCloud(\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    background_color='white'\n",
        ").generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"WordCloud Keseluruhan Teks\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyaTYG0xJXAu"
      },
      "outputs": [],
      "source": [
        "#word plot setiap label\n",
        "labels = df['hf_final'].unique()\n",
        "for label in labels:\n",
        "    words = \" \".join(df[df['hf_final'] == label]['stemmed'])\n",
        "    wc = WordCloud(width=1600, height=800, background_color='white').generate(words)\n",
        "\n",
        "    plt.figure(figsize=(14,7))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"WordCloud Label: {label}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD_mgCvtJli4"
      },
      "outputs": [],
      "source": [
        "# word plot kata paling sering muncul\n",
        "from collections import Counter\n",
        "\n",
        "# pecah token\n",
        "tokens = df['stemmed'].str.split().sum()\n",
        "\n",
        "# hitung 20 kata paling sering muncul\n",
        "word_freq = Counter(tokens).most_common(20)\n",
        "\n",
        "words, counts = zip(*word_freq)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"20 Kata Paling Sering Muncul\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddvBfoooh9qm"
      },
      "source": [
        "#Featur Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHdN8D_cwz-"
      },
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6QEbNWHQJGh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo0d99ufo0jh"
      },
      "outputs": [],
      "source": [
        "text = df['stopword'].astype(str)\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1,2),\n",
        "    stop_words=None\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPK1vLmurNcD"
      },
      "outputs": [],
      "source": [
        "# tampilkan ke dalam bentuk datframe\n",
        "tfidf_df = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=tfidf.get_feature_names_out()\n",
        ")\n",
        "\n",
        "tfidf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3bo3Cldf3rz"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeC8FJBujGZN"
      },
      "outputs": [],
      "source": [
        "# fitur = hasil TF-IDF\n",
        "X = X_tfidf\n",
        "\n",
        "# label = hasil labeling HuggingFace\n",
        "y = df['hf_final']\n",
        "\n",
        "# split 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Ukuran data latih :\", X_train.shape)\n",
        "print(\"Ukuran data uji   :\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling"
      ],
      "metadata": {
        "id": "0zYsJhwCuXLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jseSIVChkYwt"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_jVNzZHkaOZ"
      },
      "outputs": [],
      "source": [
        "y_pred = nb_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluasi"
      ],
      "metadata": {
        "id": "LaK_RMpGuhkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuKmPAMDkdUF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_8a5FcWkhlt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Prediksi data training (atau testing)\n",
        "y_pred = nb_model.predict(X_train)\n",
        "\n",
        "# 2. Buat confusion matrix\n",
        "cm = confusion_matrix(y_train, y_pred, labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "# 3. Tampilkan sebagai heatmap\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Naive Bayes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tunning"
      ],
      "metadata": {
        "id": "jmhsit04uPkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label Mapping"
      ],
      "metadata": {
        "id": "UK_xsrE0t1YF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVcuCH60KqwL"
      },
      "outputs": [],
      "source": [
        "label_map = {'Negative':0, 'Neutral':1, 'Positive':2}\n",
        "df['label_id'] = df['hf_final'].map(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Split"
      ],
      "metadata": {
        "id": "4PD6RI_6ub07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3xoZ04CK8j0"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['normalized','label_id']])\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HEMMI8OXK2yA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OR7q3ByD_wPe"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets torch accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenize"
      ],
      "metadata": {
        "id": "Etwho5k9uGqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlSqympeLE1c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
        "\n",
        "model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare Dataset"
      ],
      "metadata": {
        "id": "YmxI4I3NuntB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hfUu-ghLOuz"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch['normalized'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['normalized'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\n",
        "tokenized_dataset.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Class Weights"
      ],
      "metadata": {
        "id": "RfMaPP4lurc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# hitung berdasarkan data train\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(dataset['train']['label_id']),\n",
        "    y=dataset['train']['label_id']\n",
        ")\n",
        "\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "print(\"Class weights:\", class_weights)"
      ],
      "metadata": {
        "id": "VZSrpeCCnn_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Weighted Trainer"
      ],
      "metadata": {
        "id": "FqgLaRV3uxJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "3A0RwDUXnwyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Args"
      ],
      "metadata": {
        "id": "H3wO4cZmv1sO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h3xJdOLMDUk"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta-mrt-model\",       # folder hasil model\n",
        "    overwrite_output_dir=True,              # timpa folder jika sudah ada\n",
        "    eval_strategy=\"epoch\",                  # evaluasi tiap akhir epoch\n",
        "    save_strategy=\"epoch\",                  # simpan model tiap akhir epoch\n",
        "    learning_rate=2e-5,                     # learning rate\n",
        "    per_device_train_batch_size=16,         # batch size untuk training\n",
        "    per_device_eval_batch_size=16,          # batch size untuk evaluasi\n",
        "    num_train_epochs=4,                     # jumlah epoch\n",
        "    weight_decay=0.01,                      # regularisasi\n",
        "    logging_steps=50,                       # logging tiap 50 langkah\n",
        "    load_best_model_at_end=True,            # load model terbaik otomatis\n",
        "    metric_for_best_model=\"accuracy\",       # metric untuk model terbaik\n",
        "    greater_is_better=True                  # metric yang lebih tinggi lebih baik\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metrics"
      ],
      "metadata": {
        "id": "7XC40b67wJoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02koUw-IMHNr"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, EvalPrediction\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Definisikan metric evaluasi\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = torch.argmax(torch.tensor(p.predictions), dim=1)\n",
        "    labels = torch.tensor(p.label_ids)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Model"
      ],
      "metadata": {
        "id": "GAVKq5lju353"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoEPZFeII76f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate"
      ],
      "metadata": {
        "id": "I3RmJsQavZmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbpflZ1DMLEW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict"
      ],
      "metadata": {
        "id": "VOrQCn6tvdIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_dataset['test'])"
      ],
      "metadata": {
        "id": "QA-7AKBnsMnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_true = predictions.label_ids\n"
      ],
      "metadata": {
        "id": "0c6W6qsdsR6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification Report"
      ],
      "metadata": {
        "id": "_8f7rEofu8jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    target_names=['negative', 'neutral', 'positive']\n",
        "))"
      ],
      "metadata": {
        "id": "cKQRk2Cmsbhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix"
      ],
      "metadata": {
        "id": "On1yA-ubvnmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='d', cmap='Blues',\n",
        "    xticklabels=['negative','neutral','positive'],\n",
        "    yticklabels=['negative','neutral','positive']\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_PLzmyKsgDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "ZiDLpT-WmCLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes results (from previous execution)\n",
        "y_true_nb = y_test # The y_test is still available from the Naive Bayes split\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "print(\"Naive Bayes Classification Report (Test Set):\")\n",
        "print(classification_report(y_true_nb, y_pred_nb))\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_score(y_true_nb, y_pred_nb):.4f}\\n\")\n",
        "\n",
        "# Fine-tuned BERT results (from previous execution)\n",
        "y_true_bert = y_true # Use the in-memory variable\n",
        "y_pred_bert = y_pred # Use the in-memory variable\n",
        "\n",
        "if 'label2id' not in globals():\n",
        "    print(\"label2id not found, attempting to reconstruct...\")\n",
        "    # This is a fallback, ideally label2id would persist or be saved/loaded.\n",
        "    unique_labels = sorted(list(set(df['hf_final'].unique())))\n",
        "    le = LabelEncoder()\n",
        "    le.fit(unique_labels)\n",
        "    label2id = {l:i for i,l in enumerate(le.classes_)}\n",
        "    id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "print(\"Fine-tuned BERT Classification Report (Test Set):\")\n",
        "print(classification_report(\n",
        "    y_true_bert,\n",
        "    y_pred_bert,\n",
        "    target_names=list(label2id.keys())\n",
        "))\n",
        "print(f\"Fine-tuned BERT Accuracy: {accuracy_score(y_true_bert, y_pred_bert):.4f}\")"
      ],
      "metadata": {
        "id": "3b20-RExl4DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Naive Bayes Confusion Matrix\n",
        "cm_nb = confusion_matrix(y_true_nb, y_pred_nb, labels=['Negative', 'Neutral', 'Positive'])\n",
        "disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb,\n",
        "                                 display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "disp_nb.plot(cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix Naive Bayes (Test Set)\")\n",
        "\n",
        "# Fine-tuned BERT Confusion Matrix\n",
        "cm_bert = confusion_matrix(y_true_bert, y_pred_bert)\n",
        "disp_bert = ConfusionMatrixDisplay(confusion_matrix=cm_bert,\n",
        "                                   display_labels=list(label2id.keys()))\n",
        "disp_bert.plot(cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix Fine-tuned BERT (Test Set)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IqlMCwYKl7ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save untuk Perbandingan KRL"
      ],
      "metadata": {
        "id": "l5Z9t6Axat9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"mrt_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "Nd60_vdM4Gp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)"
      ],
      "metadata": {
        "id": "5nSGR-wc6K8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"mrt_y_true.npy\", y_true)\n",
        "np.save(\"mrt_y_pred.npy\", y_pred)"
      ],
      "metadata": {
        "id": "czauDJo96NoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "azVkdT25c3W7",
        "J1oZUWWpeJnE",
        "ddvBfoooh9qm",
        "O3bo3Cldf3rz",
        "0zYsJhwCuXLn",
        "LaK_RMpGuhkh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}