{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhidayanti/Kelompok-14-Pemrosesan-Teks/blob/main/Final/KRL/KRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRACliLdwLSu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3_ge4niawQ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('ScrappingKRL.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/KRL.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "KIzZtwK0wSFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['full_text']]\n",
        "df"
      ],
      "metadata": {
        "id": "aLPobUnsyrPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 1"
      ],
      "metadata": {
        "id": "iakyTlCNyzVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## punctuation"
      ],
      "metadata": {
        "id": "DP-4HbN8y3Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_basic(text):\n",
        "    text = str(text).lower()                             # ubah ke huruf kecil\n",
        "    text = re.sub(r'@\\w+', '', text)                     # hapus mention\n",
        "    text = re.sub(r'(?:http?://|https?://|www\\.)\\S+', '', text) # hapus url\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)          # hapus simbol aneh\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)             # ubah huruf berulang: \"heellooo\" -> \"helo\"\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)                 # hapus tanda baca, tapi simpan spasi\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()             # rapikan spasi berlebih\n",
        "    return text\n",
        "df['punctuation_text'] = df['full_text'].apply(normalize_basic)\n",
        "df_punct_view = df[['punctuation_text']]\n",
        "df_punct_view"
      ],
      "metadata": {
        "id": "w_RCj03Lweme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize"
      ],
      "metadata": {
        "id": "ALgsKWsedBRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "id": "cqrN_adSzhbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "translator = GoogleTranslator(source='en', target='id')\n",
        "\n",
        "def translate_sentence(text):\n",
        "    text = str(text)\n",
        "    try:\n",
        "        # translate seluruh kalimat sekaligus\n",
        "        translated_text = translator.translate(text)\n",
        "    except:\n",
        "        translated_text = text\n",
        "    return translated_text\n",
        "\n",
        "# Terapkan ke dataframe\n",
        "df['translated'] = df['punctuation_text'].apply(translate_sentence)\n",
        "df_translated_view = df[['punctuation_text', 'translated']]\n",
        "\n",
        "df_translated_view\n"
      ],
      "metadata": {
        "id": "lQ1m3zQT21A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_translated_view.to_csv('translated.csv', index=False)"
      ],
      "metadata": {
        "id": "al2FouxzXdtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kamus slang\n",
        "kamus = pd.read_csv('/content/drive/MyDrive/colloquial-indonesian-lexicon.csv', usecols=['slang', 'formal'])\n",
        "kamus_dict = dict(zip(kamus['slang'], kamus['formal']))\n",
        "\n",
        "kamus"
      ],
      "metadata": {
        "id": "ISA2kgYT7NUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_slang(text):\n",
        "    words = text.lower().split()\n",
        "    return ' '.join([kamus_dict.get(w, w) for w in words])\n",
        "\n",
        "df['normalized'] = df['translated'].apply(normalize_slang)\n",
        "df_normalized_view = df[['translated', 'normalized']]\n",
        "df_normalized_view"
      ],
      "metadata": {
        "id": "jlrWjxRdCXjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_normalized_view.to_csv('normalized.csv', index=False)"
      ],
      "metadata": {
        "id": "V7tZX5Zg5AFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling"
      ],
      "metadata": {
        "id": "xR4A29b0zEah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "obpaDq_ZzAIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\",\n",
        "    tokenizer=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\"\n",
        ")"
      ],
      "metadata": {
        "id": "cCPp3N3wzD__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hf_label'] = df['normalized'].apply(lambda x: sentiment_pipe(x)[0]['label'])\n",
        "df['hf_score'] = df['normalized'].apply(lambda x: sentiment_pipe(x)[0]['score'])\n",
        "df_view = df[['normalized', 'hf_label', 'hf_score']]\n",
        "df_view"
      ],
      "metadata": {
        "id": "QrMzlHzHzJyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"HuggingFace label counts:\")\n",
        "print(df['hf_label'].value_counts())"
      ],
      "metadata": {
        "id": "2r4I5o_2zKSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membaca file labeling final dari drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/LABELING FIX.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "DReX6zr9qog6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# menghitung jumlah sentimen neutral, negatif, dan positif\n",
        "print(\"Label counts:\")\n",
        "print(df['hf_final'].value_counts())"
      ],
      "metadata": {
        "id": "eG6fs0m3qvNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 2"
      ],
      "metadata": {
        "id": "X5u-AkjL5Nn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer (memisah kalimat menjadi tiap kata)"
      ],
      "metadata": {
        "id": "k4gKC1xsy-E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens'] = df['normalized'].apply(lambda x: x.split())\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [t for t in x if len(t) > 1])\n",
        "df_tokenized_view = df[['normalized', 'tokens']]\n",
        "df_tokenized_view"
      ],
      "metadata": {
        "id": "g0TuXjNKrr8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword"
      ],
      "metadata": {
        "id": "FtwVr3ahdyni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melakukan stopword manual dari kolom tokens\n",
        "import ast\n",
        "manual_stopwords = [\n",
        "    'yang', 'dan', 'di', 'ke', 'dari', 'itu', 'ini',\n",
        "    'untuk', 'pada', 'dengan', 'karena', 'bahwa', 'saat',\n",
        "    'ada', 'tidak', 'ya', 'nih', 'loh', 'sih', 'dong', 'agar', 'atau',\n",
        "    'sehingga', 'telah', 'sudah', 'tersebut', 'nya', 'lah', 'pun',\n",
        "    'seperti', 'sebuah', 'seorang', 'akan', 'para', 'dah', 'kek', 'jg', 'juga',\n",
        "    'udah', 'udahh', 'belum', 'blm',\n",
        "    'ngapa', 'ngapain', 'gimana', 'kenapa'\n",
        "]\n",
        "\n",
        "df['stopwords'] = df['tokens'].apply(lambda x: [t for t in x if t not in manual_stopwords])\n",
        "df_stopword_view = df[['tokens', 'stopwords']]\n",
        "df_stopword_view\n"
      ],
      "metadata": {
        "id": "0V2aiYWOt2pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming (Merubah Kata menjadi kata dasar)"
      ],
      "metadata": {
        "id": "EA3gpAYzeboT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "fd5PQF7LwqfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "TXwRCifYxs1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_stemming(tokens):\n",
        "  if isinstance(tokens, str):\n",
        "    tokens = tokens.split()\n",
        "  stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "  return ' '.join(stemmed_tokens)\n",
        "\n",
        "df['stemmed'] = df['stopwords'].apply(apply_stemming)\n",
        "df_stemmed_view = df[['stopwords', 'stemmed']]\n",
        "df_stemmed_view"
      ],
      "metadata": {
        "id": "R8_8RALbxupe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word cloud"
      ],
      "metadata": {
        "id": "43S7gOtY0AVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Cloud KRL\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = ' '.join(df['stemmed'])\n",
        "\n",
        "wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud KRL')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDY1HMEE0IZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word cloud per sentimen\n",
        "for label in df['hf_final'].unique():\n",
        "    text = ' '.join(df[df['hf_final'] == label]['stemmed'])\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(label)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "_OGMdhta0_Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribusi sentimen\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(x='hf_final', data=df)\n",
        "plt.title('Distribusi Sentimen')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GFjkDvXN1xF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 20 kata\n",
        "from collections import Counter\n",
        "\n",
        "all_words = sum(df['stemmed'].apply(lambda x: x.split()), [])\n",
        "word_freq = Counter(all_words).most_common(20)\n",
        "\n",
        "words, freqs = zip(*word_freq)\n",
        "plt.barh(words, freqs)\n",
        "plt.title(\"Top 20 Kata\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5RmH6UUp2Iil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "HMQMMwFcjen-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF IDF"
      ],
      "metadata": {
        "id": "OYEr3RPyng7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AjOzhR4Y3aj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['stopwords'].astype(str)\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1,2),\n",
        "    stop_words=None\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(text)\n"
      ],
      "metadata": {
        "id": "Lt2oztCr24Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=tfidf.get_feature_names_out()\n",
        ")\n",
        "\n",
        "tfidf_df"
      ],
      "metadata": {
        "id": "9wcObJWv3FRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data (data train dan data testing)"
      ],
      "metadata": {
        "id": "d-5VaPU4o4HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "GONbNBxa3VHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tfidf_df\n",
        "y = df['hf_final']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print('ukuran data latih: ', X_train.shape)\n",
        "print('ukuran data uji: ', X_test.shape)"
      ],
      "metadata": {
        "id": "JBJfyJ3A3R1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Klasifikasi Metode Naive Bayes"
      ],
      "metadata": {
        "id": "pljTwehlqvGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## modeling"
      ],
      "metadata": {
        "id": "yGZInuTc3y17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Inisialisasi dan latih model Naive Bayes\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ZZTAk2Hf3p0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data"
      ],
      "metadata": {
        "id": "qTdXqOcj30b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = nb_classifier.predict(X_train)"
      ],
      "metadata": {
        "id": "wiPKYaQu33bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Akurasi pada data training:\", accuracy_score(y_train, y_pred_train))\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "YDnxyhuN3-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi"
      ],
      "metadata": {
        "id": "HLWEdodkrbkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "vKxgzSuw5bzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Akurasi pada data testing:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "UHv-QaEi46GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cofussion matrix"
      ],
      "metadata": {
        "id": "bTSZTE8e1RkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confussion matrix data testing\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Prediksi data training (atau testing)\n",
        "y_pred = nb_classifier.predict(X_test)   # ganti X_train jadi X_test kalau mau lihat testing\n",
        "\n",
        "# 2. Buat confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "# 3. Tampilkan sebagai heatmap\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Naive Bayes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "57V4aey37T_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confussion matrix data training\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Prediksi data training (atau testing)\n",
        "y_pred = nb_classifier.predict(X_train)   # ganti X_train jadi X_test kalau mau lihat testing\n",
        "\n",
        "# 2. Buat confusion matrix\n",
        "cm = confusion_matrix(y_train, y_pred, labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "# 3. Tampilkan sebagai heatmap\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Naive Bayes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EfMqlZqgrlep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "l74RfXsX80J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data tweet KRL dilatih kembali dengan model BERT agar model lebih memahami pola bahasa dan sentimen khusus pada konteks KRL"
      ],
      "metadata": {
        "id": "pC7pVavmAZfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install dan impor"
      ],
      "metadata": {
        "id": "fdpuoMhoEn6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cCu_h0y8811T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets torch accelerate"
      ],
      "metadata": {
        "id": "WixYu-QRHk-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import httpx\n",
        "print(f\"datasets version: {datasets.__version__}\")\n",
        "print(f\"httpx version: {httpx.__version__}\")"
      ],
      "metadata": {
        "id": "wHrQOpxon-Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "jGuh5VWqEyGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data & label"
      ],
      "metadata": {
        "id": "WXO2W8UVFCIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contoh: load CSV KRL yang sudah berisi kolom 'normalized' (teks) dan 'hf_final' (label final)\n",
        "df = pd.read_csv('/content/drive/MyDrive/LABELING FIX.csv')  # sesuaikan path\n",
        "\n",
        "# pastikan nggak ada NaN di teks / label\n",
        "df = df.dropna(subset=['normalized', 'hf_final']).reset_index(drop=True)\n",
        "\n",
        "# lihat distribusi label\n",
        "print(df['hf_final'].value_counts())\n",
        "\n",
        "# encode label -> id (0..n-1)\n",
        "le = LabelEncoder()\n",
        "df['label_id'] = le.fit_transform(df['hf_final'])\n",
        "label2id = {l:i for i,l in enumerate(le.classes_)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "print(\"label2id:\", label2id)\n"
      ],
      "metadata": {
        "id": "x_r8DiJ2Eztw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split training & testing"
      ],
      "metadata": {
        "id": "3UqEeelMFEln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    df[['normalized','label_id']],\n",
        "    test_size=0.2,\n",
        "    stratify=df['label_id'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"train size:\", len(train_df), \"test size:\", len(test_df))\n"
      ],
      "metadata": {
        "id": "I0WV6cL2FLTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hitung class weight"
      ],
      "metadata": {
        "id": "QWp4f5TYFTll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute freq on training labels\n",
        "counts = train_df['label_id'].value_counts().sort_index()\n",
        "print(\"class counts (train):\", counts.to_dict())\n",
        "\n",
        "# class weights = inverse frequency\n",
        "# convert to tensor on device later\n",
        "class_freq = counts.values.astype(np.float32)\n",
        "class_weights = 1.0 / class_freq\n",
        "class_weights = class_weights / class_weights.sum() * len(class_freq)  # normalisasi (opsional)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "print(\"class weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "rF6sVhxQFPkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hugging face dataset"
      ],
      "metadata": {
        "id": "_evSx2rUFnlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"indobenchmark/indobert-base-p1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# convert ke Dataset dari pandas\n",
        "dataset = Dataset.from_pandas(train_df.rename(columns={'normalized':'text','label_id':'labels'}))\n",
        "dataset_test = Dataset.from_pandas(test_df.rename(columns={'normalized':'text','label_id':'labels'}))\n"
      ],
      "metadata": {
        "id": "k3gkyb3iFV-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "vsrnnqW6FrON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "tokenized_train = dataset.map(tokenize_fn, batched=True)\n",
        "tokenized_test = dataset_test.map(tokenize_fn, batched=True)\n",
        "\n",
        "# buang kolom teks asli agar Trainer pakai tensors\n",
        "tokenized_train = tokenized_train.remove_columns([c for c in tokenized_train.column_names if c not in ['input_ids','attention_mask','labels']])\n",
        "tokenized_test = tokenized_test.remove_columns([c for c in tokenized_test.column_names if c not in ['input_ids','attention_mask','labels']])\n",
        "\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "lGyVOdU2Fjta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model"
      ],
      "metadata": {
        "id": "VoRoGLCNFzDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "num_labels = len(label2id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "pqFLlDJgFtq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom trainer"
      ],
      "metadata": {
        "id": "hr-y9jxiF28H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Trainer: override compute_loss to include class weights\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights.to(self.model.device) if class_weights is not None else None\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "mF73dQUXF15G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training argument & trainer start"
      ],
      "metadata": {
        "id": "hoeoWnmXF9wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./indobert-krl\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")"
      ],
      "metadata": {
        "id": "JH5z1z8WF43k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_metrics (gunakan sklearn)\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "ns7AI9RmGa-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "# Mulai train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eQxnTVgTH00n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluasi"
      ],
      "metadata": {
        "id": "FPpcY6O7GMgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate(tokenized_test)\n",
        "print(\"Trainer evaluation metrics (test set):\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "Y57twv8SApN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediksi data testing"
      ],
      "metadata": {
        "id": "r962G562Av-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output = trainer.predict(tokenized_test)\n",
        "\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
        "y_true = preds_output.label_ids"
      ],
      "metadata": {
        "id": "F5n1qEheAtKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Akurasi pada data testing:\", accuracy)"
      ],
      "metadata": {
        "id": "8DO-kcWvA4wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report"
      ],
      "metadata": {
        "id": "bAOXjEZVBO1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report (test set):\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    target_names=list(label2id.keys())  # nama label sesuai mapping\n",
        "))"
      ],
      "metadata": {
        "id": "GJdDwytnBE0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## confussion matrix"
      ],
      "metadata": {
        "id": "YoNGJwB-BrRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualisasi heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=list(label2id.keys()),\n",
        "            yticklabels=list(label2id.keys()))\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "J9NgDXZmBi8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simpan model & tokenizer\n",
        "trainer.save_model(\"./indobert-krl-finetuned\")\n",
        "tokenizer.save_pretrained(\"./indobert-krl-finetuned\")"
      ],
      "metadata": {
        "id": "3I14YcuHGXu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\"krl_y_true.npy\", y_true)\n",
        "np.save(\"krl_y_pred.npy\", y_pred)\n",
        "\n",
        "print(\"File berhasil disimpan!\")\n"
      ],
      "metadata": {
        "id": "vWdL_oUnNhZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simpan file csv krl final\n",
        "df.to_csv('krl_final.csv', index=False)"
      ],
      "metadata": {
        "id": "y97qFOBOi_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5cae4c"
      },
      "source": [
        "Perbandingan metode NB dan BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4475056"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Naive Bayes results (from previous execution)\n",
        "y_true_nb = y_test # The y_test is still available from the Naive Bayes split\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "print(\"**Naive Bayes Classification Report (Test Set):**\")\n",
        "print(classification_report(y_true_nb, y_pred_nb))\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_score(y_true_nb, y_pred_nb):.4f}\\n\")\n",
        "\n",
        "# Fine-tuned BERT results (from previous execution)\n",
        "y_true_bert = np.load(\"krl_y_true.npy\")\n",
        "y_pred_bert = np.load(\"krl_y_pred.npy\")\n",
        "\n",
        "# Ensure label2id is available, or redefine if necessary\n",
        "# Assuming label2id is still in scope from previous execution\n",
        "# If not, you might need to re-run the `label2id` cell or load it.\n",
        "if 'label2id' not in globals():\n",
        "    print(\"label2id not found, attempting to reconstruct...\")\n",
        "    # This is a fallback, ideally label2id would persist or be saved/loaded.\n",
        "    unique_labels = sorted(list(set(df['hf_final'].unique())))\n",
        "    le = LabelEncoder()\n",
        "    le.fit(unique_labels)\n",
        "    label2id = {l:i for i,l in enumerate(le.classes_)}\n",
        "    id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "print(\"**Fine-tuned BERT Classification Report (Test Set):**\")\n",
        "print(classification_report(\n",
        "    y_true_bert,\n",
        "    y_pred_bert,\n",
        "    target_names=list(label2id.keys())\n",
        "))\n",
        "print(f\"Fine-tuned BERT Accuracy: {accuracy_score(y_true_bert, y_pred_bert):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "928d8f0c"
      },
      "source": [
        " Perbandingan Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd6d8f03"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Naive Bayes Confusion Matrix\n",
        "cm_nb = confusion_matrix(y_true_nb, y_pred_nb, labels=['Negative', 'Neutral', 'Positive'])\n",
        "disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb,\n",
        "                                 display_labels=['Negative', 'Neutral', 'Positive'])\n",
        "disp_nb.plot(cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix Naive Bayes (Test Set)\")\n",
        "\n",
        "# Fine-tuned BERT Confusion Matrix\n",
        "cm_bert = confusion_matrix(y_true_bert, y_pred_bert)\n",
        "disp_bert = ConfusionMatrixDisplay(confusion_matrix=cm_bert,\n",
        "                                   display_labels=list(label2id.keys()))\n",
        "disp_bert.plot(cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix Fine-tuned BERT (Test Set)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perbandingan 2 metode"
      ],
      "metadata": {
        "id": "jlViqaB1oCZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true_nb = y_test         # label sebenarnya dari test KRL\n",
        "y_pred_nb = nb_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "0yKBOOjPl8G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true_bert = np.load(\"krl_y_true.npy\")\n",
        "y_pred_bert = np.load(\"krl_y_pred.npy\")"
      ],
      "metadata": {
        "id": "jToOG2eXoG5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "label_names = [\"Negative\", \"Neutral\", \"Positive\"]"
      ],
      "metadata": {
        "id": "dmPJA5VXoKqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(\"=== Naive Bayes (KRL) ===\")\n",
        "print(classification_report(y_true_nb, y_pred_nb))\n",
        "print(\"Accuracy NB:\", accuracy_score(y_true_nb, y_pred_nb))\n",
        "\n",
        "print(\"\\n=== Fine-tuned BERT (KRL) ===\")\n",
        "print(classification_report(y_true_bert, y_pred_bert))\n",
        "print(\"Accuracy BERT:\", accuracy_score(y_true_bert, y_pred_bert))"
      ],
      "metadata": {
        "id": "OrH1WUGOoNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
        "\n",
        "# NB\n",
        "cm_nb = confusion_matrix(y_true_nb, y_pred_nb, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=labels).plot(ax=axes[0], cmap=\"Blues\")\n",
        "axes[0].set_title(\"Naive Bayes (KRL)\")\n",
        "\n",
        "# BERT\n",
        "y_true_bert_str = [id2label[label_id] for label_id in y_true_bert]\n",
        "y_pred_bert_str = [id2label[label_id] for label_id in y_pred_bert]\n",
        "\n",
        "cm_bert = confusion_matrix(y_true_bert_str, y_pred_bert_str, labels=labels)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_bert,\n",
        "                       display_labels=labels).plot(ax=axes[1], cmap=\"Blues\")\n",
        "axes[1].set_title(\"Fine-tuned BERT (KRL)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWRmfEEgoXSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ya allah jujur takut banget bantuin aku  ya allah"
      ],
      "metadata": {
        "id": "o03RSCBh4ghh"
      }
    }
  ]
}